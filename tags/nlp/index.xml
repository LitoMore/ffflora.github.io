<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Flora&#39;s</title>
    <link>https://ffflora.github.io/tags/nlp/</link>
    <description>Recent content in NLP on Flora&#39;s</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2019 22:14:01 -0800</lastBuildDate>
    
	<atom:link href="https://ffflora.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Notes on the paper (NLP): Character-level ConvNets for Text Classification</title>
      <link>https://ffflora.github.io/posts/character-level-convnet-for-text-classification/</link>
      <pubDate>Tue, 05 Nov 2019 22:14:01 -0800</pubDate>
      
      <guid>https://ffflora.github.io/posts/character-level-convnet-for-text-classification/</guid>
      <description>Background Knowledge  CNN One-hot encoding  Some highlights of the paper When trained on large-scale datasets, deep ConvNets do not require the knowledge of words, in addition to the conclusion from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language. (Chinese News corpus convert to pinyin first, and then apply this model achieves only 10 something testing error.)
Dataset size forms a dichotomy between traditional and ConvNets models: the larger datasets tend to perform better.</description>
    </item>
    
    <item>
      <title>Notes on the Paper (NLP): Skip-thought Vectors</title>
      <link>https://ffflora.github.io/posts/skip-thoughts-paper/</link>
      <pubDate>Sun, 27 Oct 2019 01:21:49 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/skip-thoughts-paper/</guid>
      <description>Paper
Code
Background Knowledge Sentence representation
Bag of words representation
Sen2Vec/Doc2Vec
Some highlights of the paper Instead of using a word to predict its surrounding context, this model encode a sentence to predict the sentences around it. The model depends on having a training corpus of contiguous text.
The model treat skip-thoughts in the framework of encoder-decoder models. That is, an encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences.</description>
    </item>
    
  </channel>
</rss>