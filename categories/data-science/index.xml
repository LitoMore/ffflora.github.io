<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Flora&#39;s</title>
    <link>https://ffflora.github.io/categories/data-science/</link>
    <description>Recent content in Data Science on Flora&#39;s</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2019 22:14:01 -0800</lastBuildDate>
    
	<atom:link href="https://ffflora.github.io/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Notes on the paper (NLP): Character-level ConvNets for Text Classification</title>
      <link>https://ffflora.github.io/posts/character-level-convnet-for-text-classification/</link>
      <pubDate>Tue, 05 Nov 2019 22:14:01 -0800</pubDate>
      
      <guid>https://ffflora.github.io/posts/character-level-convnet-for-text-classification/</guid>
      <description>Background Knowledge  CNN One-hot encoding  Some highlights of the paper When trained on large-scale datasets, deep ConvNets do not require the knowledge of words, in addition to the conclusion from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language. (Chinese News corpus convert to pinyin first, and then apply this model achieves only 10 something testing error.)
Dataset size forms a dichotomy between traditional and ConvNets models: the larger datasets tend to perform better.</description>
    </item>
    
    <item>
      <title>Notes on the Paper (NLP): Skip-thought Vectors</title>
      <link>https://ffflora.github.io/posts/skip-thoughts-paper/</link>
      <pubDate>Sun, 27 Oct 2019 01:21:49 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/skip-thoughts-paper/</guid>
      <description>Paper
Code
Background Knowledge Sentence representation
Bag of words representation
Sen2Vec/Doc2Vec
Some highlights of the paper Instead of using a word to predict its surrounding context, this model encode a sentence to predict the sentences around it. The model depends on having a training corpus of contiguous text.
The model treat skip-thoughts in the framework of encoder-decoder models. That is, an encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences.</description>
    </item>
    
    <item>
      <title>Set Up Network and HTTP Load Balancers in GCP</title>
      <link>https://ffflora.github.io/posts/network-and-http/</link>
      <pubDate>Sat, 14 Sep 2019 22:30:48 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/network-and-http/</guid>
      <description>This note is based on GCP Qwiklab Essentials.
Create multiple web server instances To simulate serving from a cluster of machines, create a simple cluster of Nginx web servers to serve static content using Instance Templates and Managed Instance Groups. Instance Templates define the look of every virtual machine in the cluster (disk, CPUs, memory, etc). Managed Instance Groups instantiate a number of virtual machine instances using the Instance Template.</description>
    </item>
    
    <item>
      <title>How to Setup Kubernetes in GCP</title>
      <link>https://ffflora.github.io/posts/kubernetes/</link>
      <pubDate>Sat, 14 Sep 2019 21:02:06 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/kubernetes/</guid>
      <description>Creating a Kubernetes Engine cluster gcloud container clusters create [CLUSTER-NAME] Get authentication credentials for the cluster gcloud container clusters get-credentials [CLUSTER-NAME] Deploying an application to the cluster use kubectl run command to create a new Deployment
kubectl run hello-server --image=gcr.io/google-samples/hello-app:1.0 --port 8080 This Kubernetes command creates a Deployment object that represents hello-app. In this command:
 --image specifies a container image to deploy. In this case, the command pulls the example image from a Google Container Registry bucket.</description>
    </item>
    
    <item>
      <title>Resnet 算法详解</title>
      <link>https://ffflora.github.io/cn/resnet/</link>
      <pubDate>Fri, 06 Sep 2019 18:08:34 -0700</pubDate>
      
      <guid>https://ffflora.github.io/cn/resnet/</guid>
      <description>ResNet 详解 The Resnet Paper is here: Deep Residual Learning for Image Recognition
Netscope CNN Analyzer
网络加深性能并不一定使模型变得更强，层数更多并不总是有更准确的结果，解决方法之一是 可以构造一个性能与浅层模型相当的结构。Residual function 顾名思义就是残差，是根据论文里 Figure2, y = F(x) + x, 需要学习的是 F, F(x) = y - x, F 就是 residual.
残差学习？残差网络？ 残差学习模块有两个分支：一是左侧的残差函数，二是右侧的对输入的恒等映射。这两个分支经过对应元素的相加之后，再经过一个非线性的变换 ReLU 激活函数，从而形成整个残差学习模块。将若干个残差模块堆叠，就成为了 残差网络。
ResNet 可以构建很深的神经网络，比如 ResNet -50/101/152 等。
ResNet - v2 ResNet 后期推出了更多其它的链接方式，但是实验之后发现还是没有 identity shortcut 好。
Pre-activation Residual Module:  原先 Residual Block 是 CBR(Conv, BN, ReLU) 结构 BRC(BN, ReLU, Conv) 结构 activate 在 conv 之前。被称作为 pre-activation 结构。  ResNeXt ResNeXt 转向对神经网络拓扑设计的研究，对 block 堆叠的策略；</description>
    </item>
    
    <item>
      <title>How to be a Kaggle Master?</title>
      <link>https://ffflora.github.io/posts/how-to-be-a-kaggle-master/</link>
      <pubDate>Thu, 05 Sep 2019 13:27:11 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/how-to-be-a-kaggle-master/</guid>
      <description>How to be a Kaggle Master Kaggle Combo x3:
Feature Engineering, Model Tuning, Model Ensemble Data Preprocessing: Missing values imputation:  Numeric Features  Impute with mean for normal distributed feature Impute with median for skewed distribution (reduce the impacts of outliers)  Categorical Features  Majority Imputation  Supervised Learning Imputation  Regression/Classification  XGBoost/LightGBM:  Missing values are gracefully treated by considering missing values as candidate for tree splitting point.</description>
    </item>
    
    <item>
      <title>Use Auto-completion in gcloud</title>
      <link>https://ffflora.github.io/posts/gcloud-and-cloud-shell/</link>
      <pubDate>Wed, 28 Aug 2019 00:02:53 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/gcloud-and-cloud-shell/</guid>
      <description>This notes is from Qwiklabs GCP Essentials.
Auto-completion gcloud interactive has auto prompting for commands and flags, and displays inline help snippets in the lower section as the command is typed.
Static information, like command and sub-command names, and flag names and enumerated flag values, are auto-completed using dropdown menus.
Install the beta components:
gcloud components install beta Enter the gcloud interactive mode:
gcloud beta interactive When using the interactive mode, click on the Tab key to complete file path and resource arguments.</description>
    </item>
    
    <item>
      <title>How to read csv files under .gz in Kaggle kernel?</title>
      <link>https://ffflora.github.io/posts/read-gz-in-kaggle/</link>
      <pubDate>Tue, 27 Aug 2019 00:23:55 -0700</pubDate>
      
      <guid>https://ffflora.github.io/posts/read-gz-in-kaggle/</guid>
      <description>It took me long time to figure it out.
Say if the files&amp;rsquo; directory in the Kaggle kernel looks like this:
competitive-data-science-predict-future-sales sales_train.csv.gz sales_train.csv When I try to list all the files in the directory, I realize that there&amp;rsquo;s no need to unzip the .gz file in order to get the .csv files, since
import os data_dir = &amp;#39;/kaggle/input/competitive-data-science-predict-future-sales&amp;#39; os.listdir(data_dir) the output is:
[&amp;#39;test.csv&amp;#39;, &amp;#39;item_categories.csv&amp;#39;, &amp;#39;sales_train.csv&amp;#39;, &amp;#39;sample_submission.csv&amp;#39;, &amp;#39;items.csv&amp;#39;, &amp;#39;shops.csv&amp;#39;] Thus in order to use the .</description>
    </item>
    
  </channel>
</rss>